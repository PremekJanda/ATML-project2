{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Topics in Machine Learning\n",
    "#### Assignment 2\n",
    "- Joona Kareinen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "file_path = 'static/audio/input_audio-id-1.wav'\n",
    "audio, sr = librosa.load(file_path, sr=16000)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Open the data file\n",
    "file_path = './data/train-v2.0.json'\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Load the data\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "\n",
    "unique_contexts = []\n",
    "contexts = []\n",
    "pairs = []\n",
    "for category in data_dict[\"data\"]:\n",
    "    for passage in category[\"paragraphs\"]:\n",
    "        context = passage[\"context\"]\n",
    "        unique_contexts.append(context)\n",
    "        for qa in passage[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            for answer in qa[\"answers\"]:\n",
    "                pairs.append([question, answer])\n",
    "                contexts.append(context)\n",
    "\n",
    "\n",
    "# Print some data\n",
    "num_titles = len(unique_contexts)\n",
    "print(f\"In the dataset there are {num_titles} different categories with total of {len(pairs)} question/answer pairs.\")\n",
    "# Test that the data was loaded correctly\n",
    "\n",
    "print(np.array(pairs[10:15]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data, and create pairs and sentences arrayas for training and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pandas.core.common import flatten\n",
    "import unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s, is_answer):\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    s = unicodeToAscii(s)\n",
    "    # Do some pruning to the data\n",
    "    s = re.sub('[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', s)\n",
    "    s = re.sub('\\W', ' ', s).lower().split()\n",
    "\n",
    "    return s\n",
    "\n",
    "tokenized_pairs = []\n",
    "tokenized_sentences = []\n",
    "for idx, pair in enumerate(pairs):\n",
    "    s1 = normalizeString(pair[0], 0)\n",
    "    s2 = normalizeString(pair[1][\"text\"], 1)\n",
    "    if len(s1) > 1 and len(s2) > 1:\n",
    "        tokenized_pairs.append([s1,s2])\n",
    "        tokenized_sentences.append(s1)\n",
    "        tokenized_sentences.append(s2)\n",
    "\n",
    "for sentence in tokenized_sentences[:10]:\n",
    "    print(sentence)\n",
    "\n",
    "for sentence in tokenized_pairs[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count words and plot the sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word2count = {}\n",
    "sen_len = []\n",
    "for sentence in tokenized_sentences:\n",
    "    sen_len.append(len(sentence))\n",
    "    for word in sentence:\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "sorted_word2vec = {k: v for k, v in sorted(word2count.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sen_len)\n",
    "plt.title(\"Histogram of the sentence lengths\")\n",
    "plt.xlabel(\"Sentence length\")\n",
    "plt.ylabel(\"Total amount\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(list(sorted_word2vec.values()), bins=50, log=True)\n",
    "plt.title(\"Average word count\")\n",
    "plt.xlabel(\"word count\")\n",
    "plt.ylabel(\"Total amount\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "model = Word2Vec(tokenized_sentences, vector_size=30, min_count=5, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "print(f\"Retrieved {len(unique_contexts)} passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_contexts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define hnswlib index path\n",
    "embeddings_cache_path = './qa_embeddings_cache.pkl'\n",
    "\n",
    "# Load cache if available\n",
    "if os.path.exists(embeddings_cache_path):\n",
    "    print('Loading embeddings cache')\n",
    "    with open(embeddings_cache_path, 'rb') as f:\n",
    "        corpus_embeddings = pickle.load(f)\n",
    "# Else compute embeddings\n",
    "else:\n",
    "    print('Computing embeddings')\n",
    "    corpus_embeddings = semb_model.encode(unique_contexts, convert_to_tensor=True, show_progress_bar=True)\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: \\'{embeddings_cache_path}\\'')\n",
    "    with open(embeddings_cache_path, 'wb') as f:\n",
    "        pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hnswlib\n",
    "import time\n",
    "start = time.time()\n",
    "# Create empthy index\n",
    "index = hnswlib.Index(space='cosine', dim=corpus_embeddings.size(1))\n",
    "\n",
    "# Define hnswlib index path\n",
    "index_path = './qa_hnswlib_100.index'\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print('Loading index...')\n",
    "    index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print('Start creating HNSWLIB index')\n",
    "    index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=100, M=64) # see https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md for parameter description\n",
    "    # Compute the HNSWLIB index (it may take a while)\n",
    "    index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(f'Saving index to: {index_path}')\n",
    "    index.save_index(index_path)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Exectution time: {int((end - start) / 60)}:{int((end - start) % 60)} min:sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Translate the following sentence from Italian to English: \"Amo la pizza\"'\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=32)\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_pipeline(\n",
    "    question,\n",
    "    similarity_model=semb_model,\n",
    "    embeddings_index=index,\n",
    "    re_ranking_model=xenc_model,\n",
    "    generative_model=model,\n",
    "    device=device\n",
    "):\n",
    "    if not question.endswith('?'):\n",
    "        question = question + '?'\n",
    "    # Embed question\n",
    "    question_embedding = similarity_model.encode(question, convert_to_tensor=True)\n",
    "    # Search documents similar to question in index\n",
    "    corpus_ids, distances = embeddings_index.knn_query(question_embedding.cpu(), k=64)\n",
    "    # Re-rank results\n",
    "    xenc_model_inputs = [(question, unique_contexts[idx]) for idx in corpus_ids[0]]\n",
    "    cross_scores = re_ranking_model.predict(xenc_model_inputs)\n",
    "    # Get best matching passage\n",
    "    passage_idx = np.argsort(-cross_scores)[0]\n",
    "    passage = unique_contexts[corpus_ids[0][passage_idx]]\n",
    "    # Encode input\n",
    "    input_text = f\"Given the following passage, answer the related question.\\n\\nPassage:\\n\\n{passage}\\n\\nQ: {question}\"\n",
    "    print('INPUT TEXT:', input_text, \"\\n\")\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Generate output\n",
    "    output_ids = generative_model.generate(input_ids, max_new_tokens=512)\n",
    "    # Decode output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return result\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Ask a question >>> \")  # e.g., \"How many fingers in a hand?\", \"What is the oldest newspaper in Chile?\", ...\n",
    "print()\n",
    "\n",
    "print(qa_pipeline(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
